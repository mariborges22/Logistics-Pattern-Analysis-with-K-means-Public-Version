{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrN8p9vOGB3St5x6VMjnKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariborges22/Logistics-Pattern-Analysis-with-K-means-Public-Version/blob/master/Segmenta%C3%A7%C3%A3o_de_produtos_do_estoque_com_K_means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UPo5ylutuo_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xLN2rJXt8kD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/BI_MOV_SUPORTE.xlsx\", engine='calamine')\n",
        "df.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-calamine"
      ],
      "metadata": {
        "id": "DfpTnoOeu8jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Verificar duplicatas\n",
        "num_duplicatas = df.duplicated().sum()\n",
        "print(f\"Duplicatas encontradas: {num_duplicatas}\")\n",
        "\n",
        "# 3. Remover espaços em branco\n",
        "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "# 4. Verificar células vazias (ou com apenas espaços)\n",
        "celulas_vazias = df.applymap(lambda x: isinstance(x, str) and x.strip() == \"\").sum()\n",
        "print(\"\\nCélulas vazias ou com espaços por coluna:\")\n",
        "print(celulas_vazias)\n",
        "\n",
        "# 5. Verificar valores únicos por coluna\n",
        "print(\"\\nValores únicos por coluna:\")\n",
        "print(df.nunique())\n",
        "\n",
        "# 6. Informações gerais\n",
        "print(\"\\nResumo do DataFrame:\")\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "uquV-_yVwQY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1ba01e"
      },
      "source": [
        "# Remove rows where 'ano_movimentacao' is 2302\n",
        "df = df[df['ano_movimentacao'] != 2302].copy()\n",
        "\n",
        "# Verify the removal\n",
        "print(\"Checking df for ano_movimentacao == 2302 after removal:\")\n",
        "display(df[df['ano_movimentacao'] == 2302])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f176b88"
      },
      "source": [
        "df_movement = df[['categoria_produto', 'subcategoria_produto', 'nomelocalestoque', 'filial', 'ano_movimentacao', 'qte_movimentacao', 'nomeempresa', 'tipo_movimentacao']].copy()\n",
        "display(df_movement.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2LgY1gn1sak"
      },
      "source": [
        "df_movement['qte_movimentacao'] = pd.to_numeric(df_movement['qte_movimentacao'], errors='coerce')\n",
        "df_aggregated_movement = df_movement.groupby(['categoria_produto', 'subcategoria_produto', 'nomelocalestoque', 'filial', 'ano_movimentacao', 'nomeempresa', 'tipo_movimentacao'])['qte_movimentacao'].sum().reset_index()\n",
        "display(df_aggregated_movement.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5e7e855"
      },
      "source": [
        "# Check for the year 2302 in the aggregated dataframe\n",
        "print(\"Checking df_aggregated_movement for ano_movimentacao == 2302:\")\n",
        "display(df_aggregated_movement[df_aggregated_movement['ano_movimentacao'] == 2302])\n",
        "\n",
        "# If found in aggregated data, check the original dataframe\n",
        "print(\"\\nChecking original df for ano_movimentacao == 2302:\")\n",
        "display(df[df['ano_movimentacao'] == 2302])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf82d991"
      },
      "source": [
        "df_sorted_movement = df_aggregated_movement.sort_values(by='tipo_movimentacao', ascending=False)\n",
        "display(df_sorted_movement.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f482d92"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_sorted_movement['category_subcategory'] = df_sorted_movement['categoria_produto'] + ' - ' + df_sorted_movement['subcategoria_produto']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='category_subcategory', y='qte_movimentacao', data=df_sorted_movement.head(10))\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Top 10 Product Categories and Subcategories by Movement')\n",
        "plt.xlabel('Category - Subcategory')\n",
        "plt.ylabel('Total Movement Quantity')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "559f5d89"
      },
      "source": [
        "pivot_table = df_sorted_movement.head(10).pivot_table(index='categoria_produto', columns=['filial', 'nomelocalestoque', 'tipo_movimentacao'], values='qte_movimentacao', fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
        "plt.title('Movement Patterns of Top Categories/Subcategories by Branch and Location')\n",
        "plt.xlabel('Branch - Location')\n",
        "plt.ylabel('Category - Subcategory')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select only numeric columns for scaling\n",
        "df_numeric = df.select_dtypes(include=['number'])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(df_numeric)"
      ],
      "metadata": {
        "id": "lRRyeUvUxV-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26027e12"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertia = []\n",
        "# Try a range of cluster numbers to find the optimal value using the elbow method\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.plot(range(1, 11), inertia)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select numeric columns from the aggregated dataframe for clustering\n",
        "df_aggregated_numeric = df_aggregated_movement.select_dtypes(include=['number'])\n",
        "\n",
        "# Scale the numeric data in the aggregated dataframe\n",
        "scaler_agg = MinMaxScaler()\n",
        "data_aggregated_scaled = scaler_agg.fit_transform(df_aggregated_numeric)\n",
        "\n",
        "# Apply KMeans clustering to the scaled aggregated data\n",
        "kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
        "kmeans.fit(data_aggregated_scaled)\n",
        "\n",
        "# Add the cluster labels to the aggregated dataframe\n",
        "df_aggregated_movement['cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "5MWYHqbm-6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e607650f"
      },
      "source": [
        "# Analyze cluster characteristics for categorical features\n",
        "\n",
        "def display_cluster_characteristics(cluster_df, cluster_label):\n",
        "    \"\"\"Displays the characteristics of a given cluster.\"\"\"\n",
        "    print(f\"\\n--- Características do Cluster {cluster_label} ---\")\n",
        "\n",
        "    print(\"\\nQuantidade Movimentada:\")\n",
        "    display(cluster_df['qte_movimentacao'].describe())\n",
        "\n",
        "    print(\"\\nCategorias de Produto mais frequentes:\")\n",
        "    display(cluster_df['categoria_produto'].value_counts().head())\n",
        "\n",
        "    print(\"\\nSubcategorias de Produto mais frequentes:\")\n",
        "    display(cluster_df['subcategoria_produto'].value_counts().head())\n",
        "\n",
        "    print(\"\\nFiliais mais frequentes:\")\n",
        "    display(cluster_df['filial'].value_counts().head())\n",
        "\n",
        "    print(\"\\nLocais de Estoque mais frequentes:\")\n",
        "    display(cluster_df['nomelocalestoque'].value_counts().head())\n",
        "\n",
        "    print(\"\\nAnos de Movimentação mais frequentes:\")\n",
        "    display(cluster_df['ano_movimentacao'].value_counts().head())\n",
        "\n",
        "    print(\"\\nEmpresas de Movimentação mais frequentes:\")\n",
        "    display(cluster_df['nomeempresa'].value_counts().head())\n",
        "\n",
        "    print(\"\\nTipos de Movimentação mais frequentes:\")\n",
        "    display(cluster_df['tipo_movimentacao'].value_counts().head())\n",
        "\n",
        "\n",
        "\n",
        "# Iterate through each unique cluster and display its characteristics\n",
        "for cluster_label in df_aggregated_movement['cluster'].unique():\n",
        "    cluster_df = df_aggregated_movement[df_aggregated_movement['cluster'] == cluster_label]\n",
        "    display_cluster_characteristics(cluster_df, cluster_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00922d30"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select only the specified numeric columns\n",
        "df_numeric_selected = df[['ano_movimentacao', 'qte_movimentacao']].copy()\n",
        "\n",
        "# Ensure 'qte_movimentacao' is numeric, coercing errors\n",
        "df_numeric_selected['qte_movimentacao'] = pd.to_numeric(df_numeric_selected['qte_movimentacao'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values that might have resulted from coercion\n",
        "df_numeric_selected.dropna(subset=['qte_movimentacao'], inplace=True)\n",
        "\n",
        "# Display dtypes before scaling to confirm\n",
        "print(\"Data types of selected numeric columns before scaling:\")\n",
        "print(df_numeric_selected.dtypes)\n",
        "\n",
        "# Scale the selected numeric data\n",
        "scaler_selected = MinMaxScaler()\n",
        "data_scaled_selected = scaler_selected.fit_transform(df_numeric_selected)\n",
        "\n",
        "# Apply PCA with 2 components (since there are only 2 features)\n",
        "pca_selected = PCA(n_components=2)\n",
        "pca_components_selected = pca_selected.fit_transform(data_scaled_selected)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_selected.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Explained Variance by PCA Components (Selected Features)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Display the explained variance ratio per component\n",
        "print(\"Explained Variance Ratio per Component (Selected Features):\", pca_selected.explained_variance_ratio_)\n",
        "print(\"Cumulative Explained Variance Ratio (Selected Features):\", np.cumsum(pca_selected.explained_variance_ratio_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8dc5805"
      },
      "source": [
        "# Apply One-Hot Encoding to 'tipo_movimentacao'\n",
        "df_encoded = pd.get_dummies(df, columns=['tipo_movimentacao'], drop_first=False)\n",
        "\n",
        "# Display the first few rows with the new encoded columns\n",
        "display(df_encoded.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141e6535"
      },
      "source": [
        "# Select the numeric columns and the one-hot encoded columns\n",
        "features_for_pca = ['ano_movimentacao', 'qte_movimentacao', 'tipo_movimentacao_ENTRADA', 'tipo_movimentacao_SAIDA']\n",
        "df_pca_features = df_encoded[features_for_pca].copy()\n",
        "\n",
        "# Ensure 'qte_movimentacao' is numeric, coercing errors\n",
        "df_pca_features['qte_movimentacao'] = pd.to_numeric(df_pca_features['qte_movimentacao'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values that might have resulted from coercion\n",
        "df_pca_features.dropna(subset=['qte_movimentacao'], inplace=True)\n",
        "\n",
        "\n",
        "# Display dtypes before scaling to confirm\n",
        "print(\"Data types of features for PCA before scaling:\")\n",
        "print(df_pca_features.dtypes)\n",
        "\n",
        "\n",
        "# Scale the selected features\n",
        "scaler_pca = MinMaxScaler()\n",
        "data_scaled_pca = scaler_pca.fit_transform(df_pca_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "440a70c5"
      },
      "source": [
        "# Apply PCA (choose number of components, e.g., 3 or based on explained variance)\n",
        "# Let's start with 3 components as we have 4 features now\n",
        "pca_full = PCA(n_components=3)\n",
        "pca_components_full = pca_full.fit_transform(data_scaled_pca)\n",
        "\n",
        "# Display the explained variance ratio\n",
        "print(\"Explained Variance Ratio per Component (Full Features):\", pca_full.explained_variance_ratio_)\n",
        "print(\"Cumulative Explained Variance Ratio (Full Features):\", np.cumsum(pca_full.explained_variance_ratio_))\n",
        "\n",
        "# Get the principal components (eigenvectors)\n",
        "pca_loadings_full_df = pd.DataFrame(pca_full.components_, columns=features_for_pca)\n",
        "\n",
        "# Display the loadings\n",
        "print(\"\\nPCA Loadings (Full Features):\")\n",
        "display(pca_loadings_full_df)\n",
        "\n",
        "# Add PCA components to a DataFrame for easier plotting, aligning with the original index\n",
        "pca_df = pd.DataFrame(pca_components_full, index=df_pca_features.index, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# Merge with the original DataFrame to get the 'ano_movimentacao' for coloring\n",
        "# Ensure the indices are aligned. df_pca_features already aligns with df_encoded and df\n",
        "pca_df_with_year = pca_df.join(df_encoded['ano_movimentacao'])\n",
        "\n",
        "# Visualize the data in the PCA space, colored by 'ano_movimentacao'\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(pca_df_with_year['PC1'], pca_df_with_year['PC2'], c=pca_df_with_year['ano_movimentacao'], cmap='viridis', alpha=0.5) # Use alpha for better visibility\n",
        "plt.title('Data Points in 2D PCA Space (PC1 vs PC2) colored by Year')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='Year') # Add a color bar to show the mapping of colors to years\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with the original DataFrame to get the 'tipo_movimentacao' for coloring\n",
        "# Ensure the indices are aligned. pca_df aligns with df\n",
        "pca_df_with_type = pca_df.join(df['tipo_movimentacao'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Use the 'tipo_movimentacao' column for coloring\n",
        "scatter = plt.scatter(pca_df_with_type['PC1'], pca_df_with_type['PC2'], c=pca_df_with_type['tipo_movimentacao'].astype('category').cat.codes, cmap='viridis', alpha=0.5) # Convert to category codes for coloring\n",
        "plt.title('Data Points in 2D PCA Space (PC1 vs PC2) colored by Movement Type')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "# Add a color bar with labels for movement types\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_ticks(range(len(pca_df_with_type['tipo_movimentacao'].unique())))\n",
        "cbar.set_ticklabels(pca_df_with_type['tipo_movimentacao'].unique())\n",
        "cbar.set_label('Movement Type')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GgyYg2fLe-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9699a797"
      },
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming you have already determined the optimal number of clusters (e.g., 6 from the elbow method)\n",
        "# and have your scaled aggregated data in 'data_aggregated_scaled' and your KMeans model in 'kmeans'\n",
        "\n",
        "# Get the cluster labels from the fitted KMeans model\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Calculate the Davies-Bouldin Index\n",
        "db_score_aggregated = davies_bouldin_score(data_aggregated_scaled, cluster_labels)\n",
        "\n",
        "print(f\"Davies-Bouldin Index for the aggregated data with 6 clusters: {db_score_aggregated}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54126008"
      },
      "source": [
        "# Product Movement Pattern Analysis - In-Depth Methodology for Data Scientists/ML Engineers\n",
        "\n",
        "This document provides a comprehensive technical breakdown of the methodology employed in this project, tailored for a Data Scientist or ML Engineer audience. It delves into the rationale behind each step and the selection of specific techniques, emphasizing the connection to the core business rule of identifying areas of significant product movement.\n",
        "\n",
        "## 1. Data Ingestion and Initial Assessment\n",
        "\n",
        "The process began with ingesting the product movement dataset. A critical initial assessment was performed to understand the data's schema, volume, and potential data quality issues. This involved:\n",
        "\n",
        "*   **Schema Inspection:** Examining column names, data types, and their potential relevance to product movement analysis (e.g., product category, stock location name, movement quantity, movement type).\n",
        "*   **Volume Assessment:** Understanding the number of records to gauge the computational resources required for subsequent processing.\n",
        "*   **Initial Quality Checks:** Quick scans for obvious data anomalies, such as unexpected data types, out-of-range values, or inconsistent formatting.\n",
        "\n",
        "The objective here was to establish a solid understanding of the raw data's landscape before any transformation.\n",
        "\n",
        "## 2. Data Wrangling and Feature Engineering\n",
        "\n",
        "This stage focused on transforming the raw data into a format suitable for analysis and feature engineering to capture relevant aspects of product movement:\n",
        "\n",
        "*   **Handling Erroneous Records:** Specifically addressing and removing records identified with improbable values, such as movement years with unlikely values (e.g., 2302), as these would distort temporal analysis and downstream modeling. The decision to remove rather than impute or correct was based on the lack of clear business context for such outliers and their potential to negatively impact model performance.\n",
        "*   **String Normalization:** Applying string cleaning operations (like removing leading/trailing whitespace) to text columns was essential to eliminate inconsistencies. This seemingly minor step is crucial for preventing issues in grouping, filtering, and categorical encoding due to variations in spacing.\n",
        "*   **Handling Missing Values:** While the initial assessment showed no nulls, the potential for empty strings after stripping was considered. Checks for empty or whitespace-only cells provided a robust check for these cases, indicating that no empty string imputation or handling was needed for the relevant columns.\n",
        "*   **Data Type Conversion:** Ensuring that the movement quantity was treated as a numeric type (`float`) was vital for aggregation and quantitative analysis. Using a robust conversion method with error coercion allowed for graceful handling of any non-numeric entries by converting them to `NaN`, which were subsequently dropped where necessary for specific analyses requiring clean numerical data.\n",
        "\n",
        "## 3. Feature Selection and Aggregation for Business Insight\n",
        "\n",
        "Drawing a direct line to the business rule of identifying areas of high movement, key features were selected to define a \"movement event\":\n",
        "\n",
        "*   Product category, product subcategory, stock location name, branch: These dimensions define *where* and *what* was moved, providing the contextual basis for identifying significant areas.\n",
        "*   Movement year, movement type: These dimensions capture the *when* and *how* of the movement, adding temporal and directional context.\n",
        "*   Movement quantity: This is the core metric, quantifying the *magnitude* of the movement.\n",
        "\n",
        "The aggregation step (grouping by the selected dimensions and summing the movement quantity) was a deliberate choice to move from granular transaction data to a summary of total movement quantity for each unique combination of the selected dimensions. This aggregated dataset became the foundation for identifying distinct movement patterns, as it encapsulates the total flow for specific product types at specific locations and times, categorized by movement type.\n",
        "\n",
        "## 4. Feature Scaling for Distance-Based Algorithms\n",
        "\n",
        "For algorithms sensitive to the magnitude of features, such as K-Means clustering and PCA, feature scaling is a mandatory preprocessing step. The **MinMaxScaler** was chosen because it transforms features to a specific range (typically 0 to 1) without distorting the relationships between values. This is appropriate when the distribution of the original data is not necessarily Gaussian and the goal is to ensure all features contribute proportionally to distance calculations, regardless of their original scale. The scaling was applied to the numeric features relevant for clustering and PCA.\n",
        "\n",
        "## 5. Unsupervised Learning: K-Means Clustering\n",
        "\n",
        "To identify inherent groupings or \"patterns\" within the aggregated movement data, **K-Means clustering** was employed. The rationale for using K-Means is its simplicity, efficiency, and interpretability. It partitions the data into a pre-defined number of clusters based on minimizing the within-cluster sum of squares. This aligns perfectly with the goal of grouping similar movement events together based on their aggregated quantity and categorical attributes (implicitly captured through the aggregation structure and potentially through one-hot encoding for PCA).\n",
        "\n",
        "*   **Determining K (Elbow Method):** The Elbow Method is a heuristic used to estimate the optimal number of clusters. By plotting the inertia (a measure of how well the data is clustered) against the number of clusters, the \"elbow point\" indicates a good balance between the complexity of the model and the reduction in within-cluster variance. This data-driven approach avoided arbitrary selection of the number of clusters.\n",
        "*   **Cluster Analysis:** Post-clustering, a critical step was analyzing the characteristics of each resulting cluster. This involved examining the descriptive statistics of movement quantity within each cluster and the frequency distribution of the categorical features (product category, branch, etc.). This step directly links the abstract clusters back to the business context, allowing for the identification of patterns like \"high-volume movements of a specific Product Type at a specific Location\" or \"low-volume movements of another Product Type at a different Location\". A utility function was developed to automate this analysis for each cluster, facilitating rapid interpretation.\n",
        "\n",
        "## 6. Clustering Validation: Davies-Bouldin Index\n",
        "\n",
        "The **Davies-Bouldin Index (DBI)** was calculated to provide a quantitative measure of the quality of the clustering solution. The DBI is an internal validation metric that evaluates the compactness and separation of the clusters. A lower DBI value indicates better clustering, with clusters being more internally cohesive and externally distinct. The obtained very low DBI score strongly suggests that the K-Means algorithm, with the chosen number of clusters, produced a robust and meaningful partitioning of the aggregated movement data. This metric provides objective evidence supporting the validity of the identified movement patterns.\n",
        "\n",
        "## 7. Dimensionality Reduction and Feature Contribution: Principal Component Analysis (PCA)\n",
        "\n",
        "**Principal Component Analysis (PCA)** was used for two primary reasons: to reduce the dimensionality of the data while retaining most of the variance, and to understand the contribution of the original features to the principal components.\n",
        "\n",
        "*   **Rationale for PCA:** Even with a relatively small number of features after aggregation, PCA helps visualize the data in a lower-dimensional space (2D or 3D plots) while capturing the most significant patterns of variation. It also helps in understanding which features are most important in differentiating the data points.\n",
        "*   **Feature Selection for PCA:** Initially, PCA was applied to just the numeric features (movement year, movement quantity) to see their individual contribution to variance. Subsequently, **One-Hot Encoding** was applied to the movement type categorical feature to include its influence in the PCA. One-Hot Encoding is a standard technique for representing categorical variables as numerical vectors, making them suitable for algorithms like PCA that operate on numerical data. The decision to include movement type in a separate PCA run was to explicitly investigate how movement direction (entry/exit) contributes to the overall data variance alongside year and quantity.\n",
        "*   **Interpreting Explained Variance and Loadings:** Examining the explained variance ratio per component revealed how much of the total data variability was captured by each principal component. The cumulative explained variance indicated how many components were needed to explain a significant portion of the variance. The PCA loadings (eigenvectors) showed the weight of each original feature in the principal components. High absolute loading values indicate that a feature has a strong influence on that component. For instance, the loadings showed that movement year and movement type were major contributors to the principal components, confirming their importance in shaping the primary dimensions of variability in the data.\n",
        "*   **Visualization in PCA Space:** Plotting the data points in the 2D PCA space (PC1 vs PC2), colored by movement year and movement type, provided intuitive visualizations of how these features drive the separation and grouping of data points in the reduced-dimensional space. The clustering of points by year and the clear separation based on movement type in the PCA plots visually corroborated the insights gained from the cluster analysis and reinforced the importance of these features in defining distinct movement patterns.\n",
        "\n",
        "## Conclusion from a Data Science Perspective\n",
        "\n",
        "This project successfully leveraged a standard machine learning pipeline (preprocessing, feature engineering, scaling, clustering, validation, and dimensionality reduction) to extract meaningful patterns from sensitive product movement data. The selection of K-Means was justified by the need for interpretable clusters tied to business dimensions. The use of the Elbow Method and Davies-Bouldin Index provided rigorous validation for the chosen clustering solution. PCA not only aided in visualization but also provided crucial insights into the feature importance, confirming the business intuition about the significance of year and movement type. The identified clusters represent actionable segments of product movement, providing a data-driven foundation for optimizing inventory management, logistics, and strategic planning, all while respecting data confidentiality by focusing on the methodology and derived patterns rather than the raw data itself."
      ]
    }
  ]
}